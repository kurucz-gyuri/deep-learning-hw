@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Dmtr2020MuchAA,
  title={Much Ado About Nothing - Identification of Zero Copulas in Hungarian Using an NMT Model},
  author={Andrea D{\"o}m{\"o}t{\"o}r and Z. Yang and A. Nov{\'a}k},
  booktitle={LREC},
  year={2020}
}

@article{XIAO2020172,
title = "Hungarian layer: A novel interpretable neural layer for paraphrase identification",
journal = "Neural Networks",
volume = "131",
pages = "172 - 184",
year = "2020",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2020.07.024",
url = "http://www.sciencedirect.com/science/article/pii/S0893608020302653",
author = "Han Xiao",
keywords = "Neural Graph, Hungarian Layer, Paraphrase Identification",
abstract = "Paraphrase identification serves as an important topic in natural language processing while sequence alignment and matching underlie the principle of this task. Traditional alignment methods take advantage of attention mechanism. Attention mechanism, i.e. weighting technique, could pick out the most similar/dissimilar parts, but is weak in modeling the aligned unmatched parts, which are the crucial evidence to identify paraphrases. In this paper, we empower neural architecture with Hungarian algorithm to extract the aligned unmatched parts. Specifically, first, our model applies BiLSTM/BERT to encode the input sentences into hidden representations. Then, Hungarian layer leverages the hidden representations to extract the aligned unmatched parts. Last, we apply cosine similarity to metric the aligned unmatched parts for a final discrimination. Extensive experiments show that our model outperforms other baselines, substantially and significantly."
}

@misc{görög2019legal,
      title={Legal entity recognition in an agglutinating language and document connection network for EU Legislation and EU/Hungarian Case Law}, 
      author={György Görög and Péter Weisz},
      year={2019},
      eprint={1907.12280},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2020experimental,
      title={An Experimental Study of Semantic Continuity for Deep Learning Models}, 
      author={Shangxi Wu and Jitao Sang and Xian Zhao and Lizhang Chen},
      year={2020},
      eprint={2011.09789},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{10.5555/2627435.2670313,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {regularization, deep learning, model combination, neural networks}
}

@misc{agarap2019deep,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}